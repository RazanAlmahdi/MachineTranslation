# -*- coding: utf-8 -*-
"""NLP_Aicha_Hanin_Razan_MachineTranslation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c9x_NrcOnZJgSWwvtRyqlM0759Jqqx1y
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense,LSTM,TimeDistributed,RepeatVector,GRU,Embedding
from tensorflow.keras.models import Model,Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

df=pd.read_csv("/content/ara_eng.txt",delimiter="\t",names=["english","arabic"])

df.head()

english_tokenizer=Tokenizer()
english_tokenizer.fit_on_texts(df["english"])

vocab_size_english=len(english_tokenizer.word_index)

vocab_size_english

english_word_2_idx=english_tokenizer.word_index
english_idx_2_word={idx:word for word,idx in english_word_2_idx.items()}

arabic_tokenizer=Tokenizer()
arabic_tokenizer.fit_on_texts(df["arabic"])

vocab_size_arabic=len(arabic_tokenizer.word_index)+1

arabic_word_2_idx=arabic_tokenizer.word_index
arabic_idx_2_word={idx:word for word,idx in arabic_word_2_idx.items()}

token_eng=english_tokenizer.texts_to_sequences(df["english"])
token_ara=arabic_tokenizer.texts_to_sequences(df["arabic"])

padded_eng=pad_sequences(token_eng,maxlen=50,padding="post")
padded_ara=pad_sequences(token_ara,maxlen=50,padding="post")

model=Sequential()
model.add(Embedding(vocab_size_english,100,input_length=50))
model.add(tf.keras.layers.Bidirectional(LSTM(units=256)))
model.add(tf.keras.layers.RepeatVector(50))
model.add(LSTM(256,return_sequences=True))
model.add(TimeDistributed(Dense(vocab_size_arabic,activation="softmax")))

model.summary()

model.compile(loss="sparse_categorical_crossentropy",optimizer=tf.keras.optimizers.RMSprop(),metrics=["accuracy"])

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(padded_eng,padded_ara,test_size=0.2,random_state=42)

model.fit(X_train,y_train,batch_size=50,validation_split=0.2,verbose=2,epochs=10)

model.evaluate(X_test,y_test)

X_test.shape

preds = model.predict(X_test[:10])

predicts = []
for i in preds:
    predicts.append(np.argmax(i[[0]]))

print([english_idx_2_word[w] for w in X_test[10] if w != 0])
print([arabic_idx_2_word[w] for w in y_test[10] if w != 0])
print([arabic_idx_2_word[w] for w in predicts if w != 0])

preds.shape

from nltk.translate.bleu_score import corpus_bleu

import re

# Function to clean and normalize text
def clean_text(text):
    # Remove extra spaces, special characters, and repeated values
    cleaned_text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    cleaned_text = re.sub(r'[^\w\s]', '', cleaned_text)  # Remove special characters
    cleaned_text = re.sub(r'(\b\w+\b)(?=.*\1)', '', cleaned_text)  # Remove repeated words

    return cleaned_text.strip()

df["arabic_reference_cleaned"] = df["arabic"].apply(clean_text)

# Extract candidate and reference translations
candidates = [arabic_idx_2_word[w] for w in predicts if w != 0]
references = df['arabic_reference_cleaned'].tolist()[:10]

# Tokenize the sentences
candidates = [sentence.split() for sentence in candidates]
references = [[sentence.split()] for sentence in references]

# Calculate BLEU score with 2-gram without smoothing
bleu_score_value = corpus_bleu(references, candidates, weights=(0.5, 0.5), smoothing_function=None)

# Print the BLEU score
print(f"Overall BLEU Score: {bleu_score_value * 100:.2f}%")

!pip install sentencepiece transformers==4.26.1

from transformers import MarianMTModel, MarianTokenizer
import pandas as pd
import torch

# Function for translation
def translate_text(source_text, source_lang, target_lang):
    model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'
    model = MarianMTModel.from_pretrained(model_name)
    tokenizer = MarianTokenizer.from_pretrained(model_name)

    input_ids = tokenizer.encode(source_text, return_tensors="pt")

    with torch.no_grad():
        output = model.generate(input_ids)

    translated_text = tokenizer.decode(output[0], skip_special_tokens=True)

    return translated_text

# Read your DataFrame from the CSV file
df = pd.read_csv("/content/ara_eng.txt", delimiter="\t", names=["english", "arabic"])
df = df[:400]

# Choose the source and target languages
source_lang = "en"
target_lang = "ar"

# Translate the English text to Arabic and create a new column for the translations
df["arabic_translation"] = df["english"].apply(lambda x: translate_text(x, source_lang, target_lang))

df.head()

import re

# Function to clean and normalize text
def clean_text(text):
    # Remove extra spaces, special characters, and repeated values
    cleaned_text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    cleaned_text = re.sub(r'[^\w\s]', '', cleaned_text)  # Remove special characters
    cleaned_text = re.sub(r'(\b\w+\b)(?=.*\1)', '', cleaned_text)  # Remove repeated words

    return cleaned_text.strip()

# Clean and normalize reference and translation columns
df["arabic_reference_cleaned"] = df["arabic"].apply(clean_text)
df["arabic_translation_cleaned"] = df["arabic_translation"].apply(clean_text)

df["arabic_reference_cleaned"]

df["arabic_translation_cleaned"]

# Function to calculate accuracy considering partial match
def partial_accuracy(reference, translation):
    reference_tokens = set(reference.split())
    translation_tokens = set(translation.split())
    common_tokens = reference_tokens.intersection(translation_tokens)

    return len(common_tokens) > 0

# Calculate overall accuracy
correct_translations = df.apply(lambda row: partial_accuracy(row["arabic_reference_cleaned"], row["arabic_translation_cleaned"]), axis=1)
accuracy = correct_translations.sum() / len(df)

print(f"Overall Accuracy: {accuracy:.2%}")

!pip install python-Levenshtein

import Levenshtein

def calculate_similarity(original_text, comparison_text):
    distance = Levenshtein.distance(original_text.lower(), comparison_text.lower())
    max_len = max(len(original_text), len(comparison_text))
    similarity_percentage = ((max_len - distance) / max_len) * 100
    return similarity_percentage

# Calculate similarity percentages
similarity = df.apply(lambda row: calculate_similarity(row["arabic_reference_cleaned"], row["arabic_translation_cleaned"]), axis=1)

# Calculate and print the average similarity
average_similarity = similarity.mean()
print(f"Average Similarity: {average_similarity:.2f}%")

!pip install nltk

from nltk.translate.bleu_score import corpus_bleu

# Extract candidate and reference translations
candidates = df['arabic_translation_cleaned'].tolist()
references = df['arabic_reference_cleaned'].tolist()

# Tokenize the sentences
candidates = [sentence.split() for sentence in candidates]
references = [[sentence.split()] for sentence in references]

# Calculate BLEU score with 2-gram without smoothing
bleu_score_value = corpus_bleu(references, candidates, weights=(0.5, 0.5), smoothing_function=None)

# Print the BLEU score
print(f"Overall BLEU Score: {bleu_score_value * 100:.2f}%")